import argparse
import sys
import time
from pathlib import Path

import streamlit as st
from bot.client.lama_cpp_client import LamaCppClient
from bot.conversation.chat_history import ChatHistory
from bot.conversation.conversation_handler import answer, extract_content_after_reasoning
from bot.model.model_registry import get_model_settings, get_models
from helpers.log import get_logger

logger = get_logger(__name__)

# Set page config at the very beginning
st.set_page_config(page_title="Chatbot", page_icon="üí¨", initial_sidebar_state="collapsed")


@st.cache_resource()
def load_llm(model_name: str, model_folder: Path) -> LamaCppClient:
    """
    Create a LLM session object that points to the model.
    """
    try:
    model_settings = get_model_settings(model_name)
        model_path = model_folder / model_settings.file_name
        
        # Check if model exists, if not show info (auto-download will happen in LamaCppClient)
        if not model_path.exists():
            st.info(f"üì• Model file not found. Downloading automatically...")
            st.info(f"üí° This may take 10-30 minutes (~4-5 GB download). Please wait...")
        
    llm = LamaCppClient(model_folder=model_folder, model_settings=model_settings)
    return llm
    except KeyError as e:
        st.error(f"‚ùå Invalid model: {e}")
        st.info(f"üí° Available models: {', '.join(get_models())}")
        st.stop()
    except Exception as e:
        st.error(f"‚ùå Failed to load model: {str(e)}")
        st.info("üí° Please check that:")
        st.info("   1. The model file exists in the models folder")
        st.info("   2. llama-cpp-python is properly installed")
        st.info("   3. You have sufficient memory/GPU resources")
        logger.error(f"Error loading LLM: {str(e)}", exc_info=True)
        st.stop()


@st.cache_resource()
def init_chat_history(total_length: int = 2) -> ChatHistory:
    chat_history = ChatHistory(total_length=total_length)
    return chat_history


def init_page(root_folder: Path) -> None:
    left_column, central_column, right_column = st.columns([2, 1, 2])

    with left_column:
        st.write(" ")

    with central_column:
        st.image(str(root_folder / "images/bot-small.png"), use_container_width=True)
        st.markdown("""<h4 style='text-align: center; color: grey;'></h4>""", unsafe_allow_html=True)

    with right_column:
        st.write(" ")

    st.sidebar.title("Options")


@st.cache_resource
def init_welcome_message() -> None:
    with st.chat_message("assistant"):
        st.write("How can I help you today?")


def reset_chat_history(chat_history: ChatHistory) -> None:
    """
    Initializes the chat history, allowing users to clear the conversation.
    """
    clear_button = st.sidebar.button("üóëÔ∏è Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = []
        chat_history.clear()


def display_messages_from_history():
    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


def main(parameters) -> None:
    root_folder = Path(__file__).resolve().parent.parent
    model_folder = root_folder / "models"
    Path(model_folder).parent.mkdir(parents=True, exist_ok=True)

    model = parameters.model
    max_new_tokens = parameters.max_new_tokens

    init_page(root_folder)
    
    # Show loading message
    with st.spinner("Loading chatbot..."):
        try:
    llm = load_llm(model, model_folder)
    chat_history = init_chat_history(2)
        except Exception as e:
            st.error(f"‚ùå Failed to initialize chatbot: {str(e)}")
            logger.error(f"Error initializing chatbot: {str(e)}", exc_info=True)
            st.stop()
    
    reset_chat_history(chat_history)
    init_welcome_message()
    display_messages_from_history()

    # Supervise user input
    if user_input := st.chat_input("Input your question!"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": user_input})

        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(user_input)

        # Display assistant response in chat message container
        start_time = time.time()
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                with st.spinner("Thinking..."):
            for token in answer(llm=llm, question=user_input, chat_history=chat_history, max_new_tokens=max_new_tokens):
                full_response += llm.parse_token(token)
                message_placeholder.markdown(full_response + "‚ñå")
            message_placeholder.markdown(full_response)
            except Exception as e:
                error_msg = f"‚ùå Error generating response: {str(e)}"
                message_placeholder.error(error_msg)
                logger.error(f"Error generating response: {str(e)}", exc_info=True)
                st.stop()

        if llm.model_settings.reasoning:
            final_answer = extract_content_after_reasoning(full_response, llm.model_settings.reasoning_stop_tag)
            if final_answer == "":
                final_answer = "I didn't provide the answer; perhaps I can try again."
        else:
            final_answer = full_response

        message_placeholder.markdown(final_answer)
        # Add assistant response to chat history
        chat_history.append(f"question: {user_input}, answer: {final_answer}")
        st.session_state.messages.append({"role": "assistant", "content": final_answer})

        took = time.time() - start_time
        logger.info(f"\n--- Took {took:.2f} seconds ---")


def get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Chatbot")

    model_list = get_models()
    default_model = model_list[0]

    parser.add_argument(
        "--model",
        type=str,
        choices=model_list,
        help=f"Model to be used. Defaults to {default_model}.",
        required=False,
        const=default_model,
        nargs="?",
        default=default_model,
    )

    parser.add_argument(
        "--max-new-tokens",
        type=int,
        help="The maximum number of tokens to generate in the answer. Defaults to 512.",
        required=False,
        default=512,
    )

    return parser.parse_args()


# streamlit run chatbot_app.py
if __name__ == "__main__":
    try:
        args = get_args()
        main(args)
    except KeyboardInterrupt:
        st.info("üëã Chatbot stopped by user")
        sys.exit(0)
    except Exception as error:
        error_msg = f"‚ùå An error occurred: {str(error)}"
        st.error(error_msg)
        logger.error(f"An error occurred: {str(error)}", exc_info=True, stack_info=True)
        sys.exit(1)
